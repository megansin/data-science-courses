{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsjq366HA18T"
      },
      "source": [
        "# Case Study 1 : Collecting Data from Twitter\n",
        "\n",
        "Due Date: 2/1/2022, **BEFORE the beginning of class at 2:00pm EST**\n",
        "\n",
        "## **NOTE: There are *always* last minute issues submitting the case studies.  DO NOT WAIT UNTIL THE LAST MINUTE!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31lTqAzjA18W"
      },
      "source": [
        "<img src=\"https://logos-download.com/wp-content/uploads/2016/02/Twitter_Logo_new-700x569.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRnGkCSRA18X"
      },
      "source": [
        "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
        "\n",
        "    Abigail Albuquerque\n",
        "    Aria Yan\n",
        "    Isabel Herrero Estrada\n",
        "    Megan Sin\n",
        "    Sandra Phan\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxc6cgPOA18Y"
      },
      "source": [
        "**Suggested Readings:** \n",
        "* Chapter 1 and Chapter 9 of the book \"Mining the Social Web\" can help a lot if you get stuck. \n",
        "* In fact, it is intentional that many of these questions can be answered directly from there (except for question 4)!\n",
        "* The idea is to ease you into the case studies :-)\n",
        "\n",
        "**Don't forget!**\n",
        "* You will need to install the twitter library to access the Twitter API\n",
        " * pip install twitter\n",
        "* NOTE: There is a package called \"python-twitter\" which will not work with this notebook!\n",
        "\n",
        "\n",
        "** NOTE **\n",
        "* **Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-uJJaulA18Y"
      },
      "source": [
        "# Problem 1 (20 points): Sampling Twitter Data with the Search API about a certain topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWqIK-0VA18Y"
      },
      "source": [
        "* Select a topic that you are interested in, for example, \"WPI\" or \"Lady Gaga\"\n",
        "* Use Twitter Search API to sample a collection of tweets about this topic. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
        "* Store the tweets you downloaded into a local file (txt file or json file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHlfqylGDAEo",
        "outputId": "cf6a8f2d-0ae6-4428-c0d7-8e4b8b8e454d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twitter\n",
            "  Downloading twitter-1.19.3-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 20 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 30 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 40 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50 kB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: twitter\n",
            "Successfully installed twitter-1.19.3\n"
          ]
        }
      ],
      "source": [
        "! pip install twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DcAmeG9A18Z"
      },
      "outputs": [],
      "source": [
        "import twitter\n",
        "#---------------------------------------------\n",
        "# Define a Function to Login Twitter API\n",
        "def oauth_login():\n",
        "    # Prof. Paffenroth has a developer account for the class.  He will provide the Twitter access tokens for\n",
        "    # each team\n",
        "    # See https://developer.twitter.com/docs/auth/oauth for more information \n",
        "    # on Twitter's OAuth implementation.\n",
        "    \n",
        "    CONSUMER_KEY = 't9kxBUDYow0xyhlmrokijm8Sn'\n",
        "    CONSUMER_SECRET = 'HRZc8ylwbeRIqnlZ44qFKwh5ZrR0Df2TK1WaKvpJzIJHGrAwWm'\n",
        "    OAUTH_TOKEN = '571213367-MQTfjqVIK19sgUanvKtM6u2MiEJr65HSfSTgjRRf'\n",
        "    OAUTH_TOKEN_SECRET = '1kQrgm98OgIYgNYxM7X6bOWUGagJgv8fMP2fRfS3dbSDb'\n",
        "    \n",
        "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
        "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
        "    \n",
        "    twitter_api = twitter.Twitter(auth=auth)\n",
        "    return twitter_api\n",
        "\n",
        "\n",
        "twitter_api = oauth_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "q = 'superbowl'\n",
        "count = 100\n",
        "search_results = twitter_api.search.tweets(q=q, count=count, lang='en')\n",
        "\n",
        "statuses = search_results['statuses']\n",
        "\n",
        "\n",
        "for _ in range(10): \n",
        "    print(\"Length of statuses\", len(statuses))\n",
        "    try:\n",
        "        next_results = search_results['search_metadata']['next_results']\n",
        "    except KeyError: # No more results when next_results doesn't exist\n",
        "        break\n",
        "        \n",
        "    # Create a dictionary from next_results, which has the following form:\n",
        "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
        "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ])\n",
        "    \n",
        "    search_results = twitter_api.search.tweets(**kwargs)\n",
        "    statuses += search_results['statuses']\n",
        "\n",
        "status_texts = [ status['text'] \n",
        "                 for status in statuses ]\n",
        "\n",
        "screen_names = [ user_mention['screen_name'] \n",
        "                 for status in statuses\n",
        "                     for user_mention in status['entities']['user_mentions'] ]\n",
        "\n",
        "hashtags = [ hashtag['text'] \n",
        "             for status in statuses\n",
        "                 for hashtag in status['entities']['hashtags'] ]\n",
        "\n",
        "words = [ w \n",
        "          for t in status_texts \n",
        "              for w in t.split() ]\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "! ls gdrive/MyDrive/DS3010CaseStudy1Data\n",
        "\n",
        "data= json.dumps(statuses, indent=1)\n",
        "\n",
        "file = open('gdrive/MyDrive/DS3010CaseStudy1Data/superbowl.txt','w')\n",
        "file.write(data)\n",
        "file.close() \n",
        "#data saved to file, do not rerun - will change file list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UDmbfRm1Kvx",
        "outputId": "9e99ca42-72f0-4516-b1a5-4c81b510deff"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of statuses 100\n",
            "Length of statuses 200\n",
            "Length of statuses 300\n",
            "Length of statuses 400\n",
            "Length of statuses 500\n",
            "Length of statuses 599\n",
            "Length of statuses 699\n",
            "Length of statuses 799\n",
            "Length of statuses 899\n",
            "Length of statuses 999\n",
            "Mounted at /content/gdrive\n",
            "superbowl.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retweets = [status['retweet_count']\n",
        "              for status in statuses]\n",
        "print(\"search topic: \" + q)\n",
        "print(\"tweets: \" + str(len(status_texts)))\n",
        "print(\"word count: \" + str(len(words)))\n",
        "#print(json.dumps(status_texts[0:5], indent=1))\n",
        "#print(json.dumps(screen_names[0:5], indent=1))\n",
        "#print(json.dumps(hashtags[0:5], indent=1))\n",
        "#print(json.dumps(words[0:5], indent=1))\n",
        "#print(json.dumps(statuses[0], indent=4))"
      ],
      "metadata": {
        "id": "950o_oy0o7fT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204c3f84-23a1-4d9a-d0b9-d4b93fd43854"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "search topic: superbowl\n",
            "tweets: 1099\n",
            "word count: 19883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHvwKm5sA18e"
      },
      "source": [
        "# Problem 2 (20 points): Analyzing Tweets and Tweet Entities with Frequency Analysis\n",
        "\n",
        "**1. Word Count:** \n",
        "* Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets. \n",
        "* Plot a table of the top 30 words with their counts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list of stopwords to filter out\n",
        "#list taken from http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n",
        "\n",
        "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
        "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
        "stopwords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
        "stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
        "stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
        "stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
        "stopwords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
        "stopwords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
        "stopwords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
        "stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
        "stopwords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
        "stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
        "stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
        "stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
        "stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
        "stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
        "stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
        "stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
        "stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
        "stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
        "stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
        "stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
        "stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
        "stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
        "stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
        "stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
        "stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
        "stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']\n",
        "stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
        "stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
        "stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
        "stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
        "stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
        "stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
        "stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
        "stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
        "stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
        "stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
        "stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
        "stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
        "stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
        "stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
        "stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
        "stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
        "stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
        "stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
        "stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
        "stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
        "stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
        "stopwords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
        "stopwords += ['yours', 'yourself', 'yourselves']"
      ],
      "metadata": {
        "id": "Kj45r2MvEwpZ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "V_ZHKBRuA18e"
      },
      "outputs": [],
      "source": [
        "def cleanWordList(words):\n",
        "  return [w.lower() for w in words]\n",
        "\n",
        "def wordListToFreqDict(wordlist):\n",
        "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
        "    return dict(list(zip(wordlist,wordfreq)))\n",
        "\n",
        "def sortFreqDict(freqdict):\n",
        "    aux = [(freqdict[key], key) for key in freqdict]\n",
        "    aux.sort()\n",
        "    aux.reverse()\n",
        "    return aux\n",
        "\n",
        "def removeStopwords(wordlist, stopwords):\n",
        "    return [w for w in wordlist if w not in stopwords]\n",
        "\n",
        "wordlist = cleanWordList(words)\n",
        "\n",
        "wordfreq = [words.count(w) for w in wordlist]\n",
        "freqdict = wordListToFreqDict(wordlist)\n",
        "sortedfreqdict = sortFreqDict(freqdict)\n",
        "#print(sortedfreqdict[:20])\n",
        "\n",
        "#print(\"remove stopwords\")\n",
        "filtwordlist = removeStopwords(wordlist, stopwords)\n",
        "filwordfreq = [filtwordlist.count(w) for w in filtwordlist]\n",
        "filtfreqdict = wordListToFreqDict(filtwordlist)\n",
        "filtsortedfreqdict = sortFreqDict(filtfreqdict)\n",
        "#print(filtsortedfreqdict[:30])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "words = []\n",
        "freq = []\n",
        "\n",
        "for duo in filtsortedfreqdict[:30]:\n",
        "  words.append(duo[1])\n",
        "  freq.append(duo[0])\n",
        "\n",
        "data = {'Frequency': freq,\n",
        "        'Words': words}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "id": "AtejnvVgMKGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f2d9cd-c2e7-4d43-c0ec-6ed92eca8d31"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Frequency                    Words\n",
            "       625                       rt\n",
            "       337                superbowl\n",
            "       333               #superbowl\n",
            "       121                      day\n",
            "       108                  bengals\n",
            "       106                      win\n",
            "        89                     just\n",
            "        82                 santonio\n",
            "        79                       td\n",
            "        69              spectacular\n",
            "        69           roethlisberger\n",
            "        69                   linked\n",
            "        69                   holmes\n",
            "        69                    grabs\n",
            "        69                      ben\n",
            "        69               @superbow‚Ä¶\n",
            "        69             @nfllegends:\n",
            "        69                    2009,\n",
            "        64                 stafford\n",
            "        60                      joe\n",
            "        58                   burrow\n",
            "        57                      lvi\n",
            "        55                    years\n",
            "        55                     game\n",
            "        52                    super\n",
            "        52                  matthew\n",
            "        49                      vs.\n",
            "        49  https://t.co/7pzfy9cknd\n",
            "        49         @localsoundwave:\n",
            "        49           #rulethejungle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMigKzFlA18e"
      },
      "source": [
        "**2. Find the most popular tweets in your collection of tweets**\n",
        "\n",
        "Please plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "retweets = [status['retweet_count']\n",
        "              for status in statuses]\n",
        "\n",
        "\n",
        "rtdict = dict(list(zip(status_texts, retweets)))\n",
        "def sortRTdict(freqdict):\n",
        "    aux = [(freqdict[key], key) for key in freqdict]\n",
        "    aux.sort()\n",
        "    aux.reverse()\n",
        "    return aux\n",
        "sortedrtdict = sortRTdict(rtdict)\n",
        "rts = []\n",
        "tweets = []\n",
        "\n",
        "for duo in sortedrtdict[:10]:\n",
        "  tweets.append(duo[1])\n",
        "  rts.append(duo[0])\n",
        "\n",
        "printdata = {'Retweets': rts,\n",
        "        'Tweet': tweets}\n",
        "\n",
        "rtdf = pd.DataFrame(printdata)\n",
        "print(rtdf.to_string(index=False))"
      ],
      "metadata": {
        "id": "_PMhoaUsZLrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38037835-0230-4cb7-943c-d7249e434488"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Retweets                                                                                                                                             Tweet\n",
            "     6253                                         If you go from a 10 to a 5 because of light rain. Don't tell me who I can root for in the Superbowl ü§∑üèº‚Äç‚ôÇÔ∏è\n",
            "     6253       @BleacherReport @jalenramsey @brgridiron It had to be somebody. At least it was the GOAT. You can put that on your‚Ä¶ https://t.co/wIiTDnQJ2v\n",
            "     6253  #NFLPlayoffs Pass Update :\\n4% #SuperBowl BEST BET Added\\nvia @Greek_Gambler Card -\\nhttps://t.co/l13VxT2j5x \\n\\n5% Big‚Ä¶ https://t.co/4O6ZRylmMG\n",
            "     3533                                  RT @kplightning: 7 years since we got the most watched superbowl performance of all time https://t.co/kUBAEO9I6d\n",
            "     2605                                                RT @49ersSportsTalk: This is just beautiful‚Ä¶ ü•∫ü•≤\\n#Bengals #SuperBowl #NFL  https://t.co/eau6wbIA0u\n",
            "     2605      @TomBrady @Buccaneers Now that Bruce A. screwed up Tom's superbowl, everyone will see what a lousy coach BA is. The‚Ä¶ https://t.co/lnAdNXQXzq\n",
            "     2605                     @JRho_11 USC immediately outdrawing the Rams despite a Superbowl appearance and possible victory &gt;&gt;&gt;&gt;&gt;&gt;&gt;\n",
            "     2605      @Buccaneers Now that Bruce A. screwed up Tom's superbowl, everyone will see what a lousy coach BA is. The tampa bay‚Ä¶ https://t.co/vaq8ddFUpH\n",
            "     2605      @Buccaneers Now that Bruce A. screwed up Tom's superbowl, everyone will see what a lousy coach BA is. The tampa bay‚Ä¶ https://t.co/r5QR29XLCU\n",
            "     2605      @Buccaneers Now that Bruce A. screwed up Tom's superbowl, everyone will see what a lousy coach BA is. The tampa bay‚Ä¶ https://t.co/nCSduL2toF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbUOQT32A18f"
      },
      "source": [
        "**3. Find the most popular Tweet Entities in your collection of tweets**\n",
        "\n",
        "Please plot a table of the top 10 hashtags, top 10 user mentions that are the most popular in your collection of tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKYkobrzA18f"
      },
      "outputs": [],
      "source": [
        "\n",
        "#wordlist = cleanWordList(words)\n",
        "\n",
        "htfreq = [hashtags.count(ht) for ht in hashtags]\n",
        "htdict = wordListToFreqDict(hashtags)\n",
        "#print(htfreq)\n",
        "sortedhtdict = sortFreqDict(htdict)\n",
        "#print(sortedhtdict[:10])\n",
        "\n",
        "htfreq = []\n",
        "hashtags = []\n",
        "\n",
        "for duo in sortedhtdict[:10]:\n",
        "  hashtags.append(duo[1])\n",
        "  htfreq.append(duo[0])\n",
        "\n",
        "htdata = {'Hashtag Frequency': htfreq,\n",
        "        'Hashtag': hashtags}\n",
        "\n",
        "htdf = pd.DataFrame(htdata)\n",
        "print(htdf.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "umfreq = [screen_names.count(um) for um in screen_names]\n",
        "umdict = wordListToFreqDict(screen_names)\n",
        "sortedumdict = sortFreqDict(umdict)\n",
        "umfreq = []\n",
        "usermentions = []\n",
        "\n",
        "for duo in sortedumdict[:10]:\n",
        "  usermentions.append(duo[1])\n",
        "  umfreq.append(duo[0])\n",
        "\n",
        "utdata = {'User Mention Frequency': umfreq,\n",
        "          'Users': usermentions}\n",
        "\n",
        "utdf = pd.DataFrame(utdata)\n",
        "print(utdf.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "67mnEWZOyPrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLEC3fX_A18g"
      },
      "source": [
        "* ------------------------\n",
        "\n",
        "# Problem 3 (20 points): Getting \"All\" friends and \"All\" followers of a popular user in twitter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wTKbSAnA18g"
      },
      "source": [
        "* choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
        "* Get the list of all friends and all followers of the twitter user.\n",
        "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
        "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tweepy[async]"
      ],
      "metadata": {
        "id": "r0JTCp0gnaUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI8np92oA18h"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n",
        "\n",
        "import tweepy\n",
        "\n",
        "CONSUMER_KEY = 't9kxBUDYow0xyhlmrokijm8Sn'\n",
        "CONSUMER_SECRET = 'HRZc8ylwbeRIqnlZ44qFKwh5ZrR0Df2TK1WaKvpJzIJHGrAwWm'\n",
        "OAUTH_TOKEN = '571213367-MQTfjqVIK19sgUanvKtM6u2MiEJr65HSfSTgjRRf'\n",
        "OAUTH_TOKEN_SECRET = '1kQrgm98OgIYgNYxM7X6bOWUGagJgv8fMP2fRfS3dbSDb'\n",
        "\n",
        "# authorization of consumer key and consumer secret\n",
        "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "  \n",
        "# set access to user's access key and access secret \n",
        "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
        "\n",
        "# calling the api \n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# the screen_name of the targeted user\n",
        "screen_name = \"kendricklamar\"\n",
        "\n",
        "# get all friends\n",
        "friends = tweepy.Cursor(api.friends, screen_name)\n",
        "\n",
        "# get all followers\n",
        "followers = tweepy.Cursor(api.followers, screen_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lists for follower screen names and ID numbers\n",
        "follower_screen_names = []\n",
        "follower_id = []\n",
        "\n",
        "# adding the latest 20 followers of the user to lists\n",
        "for follower in api.followers(screen_name):\n",
        "  follower_screen_names.append(follower.screen_name)\n",
        "  follower_id.append(follower.id)\n",
        "\n",
        "# plot friends in table\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Screen Name': follower_screen_names,\n",
        "        'ID': follower_id}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"20 Followers:\")\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "id": "iOIyl_4wps5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lists for friend screen names and ID numbers\n",
        "friend_screen_names = []\n",
        "friend_id = []\n",
        "\n",
        "# adding the latest 20 friends of the user to lists\n",
        "for friend in api.friends(screen_name):\n",
        "  friend_screen_names.append(friend.screen_name)\n",
        "  friend_id.append(friend.id)\n",
        "\n",
        "# plot friends in table\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Screen Name': friend_screen_names,\n",
        "        'ID': friend_id}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"20 Friends:\")\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "id": "tg8Mowxvqehl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_rate_limit_error_message(error_msg):\n",
        "  raise RateLimitError(error_msg, resp)\n",
        "else:\n",
        "  raise TweepError(error_msg, resp, api_code=api_error_code)"
      ],
      "metadata": {
        "id": "qDLM-x8WzC3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGOvRwzxA18h"
      },
      "source": [
        "* Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGOM-EKTA18i"
      },
      "outputs": [],
      "source": [
        "# lists for mutual screen names and ID numbers\n",
        "mutual_screen_names = []\n",
        "mutual_id = []\n",
        "\n",
        "# loop through friends and add screen names and IDs for people who also follow back\n",
        "for friend in api.friends(screen_name):\n",
        "  status = api.show_friendship(api.get_user(screen_name).id, screen_name, friend.id, friend.screen_name)\n",
        "  if(status[1].following == True):\n",
        "    mutual_screen_names.append(status[1].screen_name)\n",
        "    mutual_id.append(status[1].id)\n",
        "\n",
        "# plot friends in table\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Screen Name': mutual_screen_names,\n",
        "        'ID': mutual_id}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Mutuals:\")\n",
        "print(df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B53GdUzA18i"
      },
      "source": [
        "*------------------------\n",
        "\n",
        "# Problem 4 (20 points): Business question \n",
        "\n",
        "Run some additional experiments with your data to gain familiarity with the twitter data and twitter API.\n",
        "\n",
        "* Come up with a business question that Twitter data could help answer.\n",
        "* Decribe the business case.\n",
        "* How could Twitter data help a company decide how to spend its resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FomaD-F0A18i"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "\n",
        "CONSUMER_KEY = 't9kxBUDYow0xyhlmrokijm8Sn'\n",
        "CONSUMER_SECRET = 'HRZc8ylwbeRIqnlZ44qFKwh5ZrR0Df2TK1WaKvpJzIJHGrAwWm'\n",
        "OAUTH_TOKEN = '571213367-MQTfjqVIK19sgUanvKtM6u2MiEJr65HSfSTgjRRf'\n",
        "OAUTH_TOKEN_SECRET = '1kQrgm98OgIYgNYxM7X6bOWUGagJgv8fMP2fRfS3dbSDb'\n",
        "\n",
        "# authorization of consumer key and consumer secret\n",
        "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "  \n",
        "# set access to user's access key and access secret \n",
        "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
        "\n",
        "# calling the api \n",
        "API = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "place = API.reverse_geocode(lat=42.1040, lon=-93.5003)\n",
        "print(place)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2yf56GY207",
        "outputId": "ec1b97e7-80b8-46f7-b429-9d078d63f36b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Place(_api=<tweepy.api.API object at 0x7f35ba85de10>, id='3cd4c18d3615bbc9', name='Iowa', full_name='Iowa, USA', country='United States', country_code='US', url='https://api.twitter.com/1.1/geo/id/3cd4c18d3615bbc9.json', place_type='admin', attributes={}, bounding_box=BoundingBox(_api=<tweepy.api.API object at 0x7f35ba85de10>, type='Polygon', coordinates=[[[-96.6396669, 40.375437], [-96.6396669, 43.50102], [-90.140061, 43.50102], [-90.140061, 40.375437], [-96.6396669, 40.375437]]]), centroid=[-93.15059479194798, 41.938431], contained_within=[Place(_api=<tweepy.api.API object at 0x7f35ba85de10>, id='96683cc9126741d1', name='United States', full_name='United States', country='United States', country_code='US', url='https://api.twitter.com/1.1/geo/id/96683cc9126741d1.json', place_type='country', attributes={}, bounding_box=BoundingBox(_api=<tweepy.api.API object at 0x7f35ba85de10>, type='Polygon', coordinates=[[[-179.231086, 13.182335], [-179.231086, 71.434357], [179.859685, 71.434357], [179.859685, 13.182335], [-179.231086, 13.182335]]]), centroid=[-98.99322144171052, 36.8908925])]), Place(_api=<tweepy.api.API object at 0x7f35ba85de10>, id='96683cc9126741d1', name='United States', full_name='United States', country='United States', country_code='US', url='https://api.twitter.com/1.1/geo/id/96683cc9126741d1.json', place_type='country', attributes={}, bounding_box=BoundingBox(_api=<tweepy.api.API object at 0x7f35ba85de10>, type='Polygon', coordinates=[[[-179.231086, 13.182335], [-179.231086, 71.434357], [179.859685, 71.434357], [179.859685, 13.182335], [-179.231086, 13.182335]]]), centroid=[-98.99322144171052, 36.8908925], contained_within=[])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import twitter\n",
        "#---------------------------------------------\n",
        "# Define a Function to Login Twitter API\n",
        "def oauth_login():\n",
        "    # Prof. Paffenroth has a developer account for the class.  He will provide the Twitter access tokens for\n",
        "    # each team\n",
        "    # See https://developer.twitter.com/docs/auth/oauth for more information \n",
        "    # on Twitter's OAuth implementation.\n",
        "    \n",
        "    CONSUMER_KEY = 't9kxBUDYow0xyhlmrokijm8Sn'\n",
        "    CONSUMER_SECRET = 'HRZc8ylwbeRIqnlZ44qFKwh5ZrR0Df2TK1WaKvpJzIJHGrAwWm'\n",
        "    OAUTH_TOKEN = '571213367-MQTfjqVIK19sgUanvKtM6u2MiEJr65HSfSTgjRRf'\n",
        "    OAUTH_TOKEN_SECRET = '1kQrgm98OgIYgNYxM7X6bOWUGagJgv8fMP2fRfS3dbSDb'\n",
        "    \n",
        "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
        "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
        "    \n",
        "    twitter_api = twitter.Twitter(auth=auth)\n",
        "    return twitter_api\n",
        "\n",
        "\n",
        "twitter_api = oauth_login()"
      ],
      "metadata": {
        "id": "i3T1yUXFsatR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "PscVSVMuA18j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef137e3-7603-4764-fe45-d2f5a83df315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of statuses 100\n",
            "Length of statuses 200\n",
            "Length of statuses 300\n",
            "Length of statuses 400\n",
            "Length of statuses 500\n",
            "Length of statuses 600\n",
            "Length of statuses 700\n",
            "Length of statuses 800\n",
            "Length of statuses 900\n",
            "Length of statuses 1000\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "q = 'rams'\n",
        "count = 100\n",
        "\n",
        "\n",
        "\n",
        "search_results = twitter_api.search.tweets(q=q, count=count, lang='en')\n",
        "\n",
        "statuses = search_results['statuses']\n",
        "\n",
        "for _ in range(10): \n",
        "    print(\"Length of statuses\", len(statuses))\n",
        "    try:\n",
        "        next_results = search_results['search_metadata']['next_results']\n",
        "    except KeyError: # No more results when next_results doesn't exist\n",
        "        break\n",
        "        \n",
        "    # Create a dictionary from next_results, which has the following form:\n",
        "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
        "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ])\n",
        "    \n",
        "    search_results = twitter_api.search.tweets(**kwargs)\n",
        "    statuses += search_results['statuses']\n",
        "\n",
        "status_texts = [ status['text'] \n",
        "                 for status in statuses ]\n",
        "\n",
        "screen_names = [ user_mention['screen_name'] \n",
        "                 for status in statuses\n",
        "                     for user_mention in status['entities']['user_mentions'] ]\n",
        "\n",
        "hashtags = [ hashtag['text'] \n",
        "             for status in statuses\n",
        "                 for hashtag in status['entities']['hashtags'] ]\n",
        "\n",
        "words = [ w \n",
        "          for t in status_texts \n",
        "              for w in t.split() ]\n",
        "\n",
        "\n",
        "#print(json.dumps(statuses[0], indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordlist = cleanWordList(words)\n",
        "\n",
        "wordfreq = [words.count(w) for w in wordlist]\n",
        "freqdict = wordListToFreqDict(wordlist)\n",
        "sortedfreqdict = sortFreqDict(freqdict)\n",
        "#print(sortedfreqdict[:20])\n",
        "\n",
        "#print(\"remove stopwords\")\n",
        "filtwordlist = removeStopwords(wordlist, stopwords)\n",
        "filwordfreq = [filtwordlist.count(w) for w in filtwordlist]\n",
        "filtfreqdict = wordListToFreqDict(filtwordlist)\n",
        "filtsortedfreqdict = sortFreqDict(filtfreqdict)\n",
        "#print(filtsortedfreqdict[:30])"
      ],
      "metadata": {
        "id": "7TRwk_3af55d"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "words = []\n",
        "freq = []\n",
        "\n",
        "for duo in filtsortedfreqdict[:30]:\n",
        "  words.append(duo[1])\n",
        "  freq.append(duo[0])\n",
        "\n",
        "data = {'Frequency': freq,\n",
        "        'Words': words}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr6Nm3CSf-m3",
        "outputId": "08342960-fa32-4fd6-f8ba-4b584fd61121"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Frequency                    Words\n",
            "       664                     rams\n",
            "       460                       rt\n",
            "       189                    super\n",
            "       177                      win\n",
            "       176            @timthetatman\n",
            "       162                    right\n",
            "       132                     bowl\n",
            "       116                    rams.\n",
            "       115                  bengals\n",
            "       104                     said\n",
            "       102                    brady\n",
            "        97                     mask\n",
            "        95                     just\n",
            "        91                     fans\n",
            "        90                   newsom\n",
            "        85                     lost\n",
            "        81                     took\n",
            "        81                    times\n",
            "        81                     feel\n",
            "        77                      100\n",
            "        76                   worse.\n",
            "        75  https://t.co/u5fu5fe1hi\n",
            "        75        @realskipbayless:\n",
            "        73                   picked\n",
            "        62                    gavin\n",
            "        55               california\n",
            "        54             @claytravis:\n",
            "        54              49ers-rams.\n",
            "        53                 governor\n",
            "        49                      is‚Ä¶\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "umfreq = [screen_names.count(um) for um in screen_names]\n",
        "umdict = wordListToFreqDict(screen_names)\n",
        "sortedumdict = sortFreqDict(umdict)\n",
        "umfreq = []\n",
        "usermentions = []\n",
        "\n",
        "for duo in sortedumdict[:10]:\n",
        "  usermentions.append(duo[1])\n",
        "  umfreq.append(duo[0])\n",
        "\n",
        "utdata = {'User Mention Frequency': umfreq,\n",
        "          'Users': usermentions}\n",
        "\n",
        "utdf = pd.DataFrame(utdata)\n",
        "print(utdf.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYzoklkCenbH",
        "outputId": "b3a719a6-2dc0-4dfb-a56f-9f6d613ebf1c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " User Mention Frequency            Users\n",
            "                    224     timthetatman\n",
            "                     78  RealSkipBayless\n",
            "                     54       ClayTravis\n",
            "                     16     weddlesbeard\n",
            "                     16          RamsNFL\n",
            "                     15  WillLesterPhoto\n",
            "                     14      ninernate49\n",
            "                     13              PFF\n",
            "                     13          Bengals\n",
            "                     13     AroundTheNFL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrHmit6A18j"
      },
      "source": [
        "*-----------------\n",
        "# Done\n",
        "\n",
        "All set! \n",
        "\n",
        "** What do you need to submit?**\n",
        "\n",
        "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
        "\n",
        "\n",
        "* **PPT Slides**: please prepare PPT slides (for 10-15 minutes talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
        "\n",
        "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
        "    * What data you collected? \n",
        "    * Why this topic is interesting or important to you? (Motivations)\n",
        "    * How did you analyse the data?\n",
        "    * What did you find in the data? \n",
        " \n",
        "     (please include figures or tables in the report, but no source code)\n",
        "\n",
        "Please compress all the files in a zipped file.\n",
        "\n",
        "\n",
        "** How to submit: **\n",
        "\n",
        "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA (dyou@wpi.edu).\n",
        "\n",
        "#### We auto-process the submissions so make sure your subject line is *exactly*:\n",
        "\n",
        "### DS3010 Case Study 1 Team ??\n",
        "\n",
        "#### where ?? is your team number.\n",
        "        \n",
        "** Note: Each team just needs to submits one submission **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_y_VpkSA18j"
      },
      "source": [
        "# Grading Criteria:\n",
        "\n",
        "** Totoal Points: 100 **\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "** Notebook results:  **\n",
        "    Points: 80\n",
        "\n",
        "\n",
        "    -----------------------------------\n",
        "    Qestion 1:\n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "    \n",
        "    (1) Select a topic that you are interested in.\n",
        "    Points: 6 \n",
        "    \n",
        "    (2) Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million. Please check whether the total number of tweets collected is larger than 200?\n",
        "    Points: 10 \n",
        "    \n",
        "    \n",
        "    (3) Store the tweets you downloaded into a local file (txt file or json file)\n",
        "    Points: 4 \n",
        "    \n",
        "    \n",
        "    -----------------------------------\n",
        "    Qestion 2:\n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "    \n",
        "    1. Word Count\n",
        "\n",
        "    (1) Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets.\n",
        "    Points: 4 \n",
        "\n",
        "    (2) Plot a table of the top 30 words with their counts \n",
        "    Points: 4 \n",
        "    \n",
        "    2. Find the most popular tweets in your collection of tweets\n",
        "    plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n",
        "    Points: 4 \n",
        "    \n",
        "    3. Find the most popular Tweet Entities in your collection of tweets\n",
        "\n",
        "    (1) plot a table of the top 10 hashtags, \n",
        "    Points: 4 \n",
        "\n",
        "    (2) top 10 user mentions that are the most popular in your collection of tweets.\n",
        "    Points: 4 \n",
        "    \n",
        "    \n",
        "    -----------------------------------\n",
        "    Qestion 3:\n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "    \n",
        "    (1) choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
        "    Points: 4 \n",
        "\n",
        "    (2) Get the list of all friends and all followers of the twitter user.\n",
        "    Points: 4 \n",
        "\n",
        "    (3) Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
        "    Points: 4 \n",
        "\n",
        "    (4) Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table.\n",
        "    Points: 4 \n",
        "    \n",
        "    (5) Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table\n",
        "    Points: 4 \n",
        "  \n",
        "    -----------------------------------\n",
        "    Qestion 4:  Business question\n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "        Novelty: 10\n",
        "        Interestingness: 10\n",
        "    -----------------------------------\n",
        "    Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API.  Come up with a business question and describe how Twitter data can help you answer that question.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Slides (for presentation): Story-telling**\n",
        "    Points: 20\n",
        "\n",
        "\n",
        "1. Motivation about the data collection, why the topic is interesting to you.\n",
        "    Points: 5 \n",
        "\n",
        "2. Communicating Results (figure/table)\n",
        "    Points: 10 \n",
        "\n",
        "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
        "    Points: 5 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rRwd24iJsr9R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "CaseStudy1_Group9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.2.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}