{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3AaPphVQtfQ"
      },
      "source": [
        "# Case Study 3 : Textual analysis of movie reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xFcGCJQtfS"
      },
      "source": [
        "**Due Date: February 22, 2022, BEFORE the beginning of class at 2:00pm ET**\n",
        "\n",
        "NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W96Ki3enQtfT"
      },
      "source": [
        "<img src=\"https://getthematic.com/wp-content/uploads/2018/03/Harris-Word-Cloud-e1522406279125.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z6U9nIKQtfT"
      },
      "source": [
        "**TEAM Members:** \n",
        "\n",
        "\n",
        "  Abigail Albuquerque\n",
        "\n",
        "  Aria Yan\n",
        "\n",
        "  Isabel Herrero Estrada\n",
        "\n",
        "  Sandra Phan\n",
        "\n",
        "  Megan Sin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeewMOzEQtfT"
      },
      "source": [
        "**Desired outcome of the case study.**\n",
        "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
        "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
        "    * It contains written reviews of movies divided into positive and negative reviews.\n",
        "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
        "    \n",
        "**Required Readings:** \n",
        "* This case study will be based upon the scikit-learn Python library\n",
        "* We will build upon the tutorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "* In particular, this case study is quite similar to \"Exercise 2: Sentiment Analysis on movie reviews\" on the above web page.\n",
        "* Read about deep learning at https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
        "\n",
        "\n",
        "**Case study assumptions:**\n",
        "* You have access to a python installation\n",
        "\n",
        "**Required Python libraries:**\n",
        "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
        "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
        "* Scikit-learn (scikit-learn.org).\n",
        "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
        "\n",
        "** NOTE **\n",
        "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data onto Colab example."
      ],
      "metadata": {
        "id": "WQQrXatF30aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ludq2Spg3zdC",
        "outputId": "0384800a-9ebf-478f-c8d3-47a06048aad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-21 21:46:16--  https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-21 21:46:16 (22.2 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look for the directory txt_sentoken"
      ],
      "metadata": {
        "id": "j6lyvK0T4HCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! tar xzf review_polarity.tar.gz\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqAfVpEJ4A0P",
        "outputId": "be55a662-9739-4f88-9a42-e9a33c669813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "poldata.README.2.0  review_polarity.tar.gz  sample_data  txt_sentoken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m-jzLoyQtfU"
      },
      "source": [
        "## Problem 1 (10 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyZeq4COQtfU"
      },
      "source": [
        "* Installing scikit-learn using Anaconda does not necessarily download the example source-code.\n",
        "* Accordingly, you may need to download these directly from Github at https://github.com/scikit-learn/scikit-learn:\n",
        "    * The data can be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
        "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
        "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
        "* Here is a direct link to the code to help you out:  https://github.com/scikit-learn/scikit-learn/tree/main/doc/tutorial/text_analytics\n",
        "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
        "\n",
        "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
        "* This will likely involve moving around data files and/or small modifications to the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9rv82MAQtfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e98e324-544d-44ec-fdae-7b877c605531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz (3 MB)\n",
            "Decompressing review_polarity.tar.gz\n"
          ]
        }
      ],
      "source": [
        "#Download the dataset\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "from contextlib import closing\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "URL = (\"http://www.cs.cornell.edu/people/pabo/\"\n",
        "       \"movie-review-data/review_polarity.tar.gz\")\n",
        "\n",
        "ARCHIVE_NAME = URL.rsplit('/', 1)[1]\n",
        "DATA_FOLDER = \"txt_sentoken\"\n",
        "\n",
        "if not os.path.exists(DATA_FOLDER):\n",
        "\n",
        "    if not os.path.exists(ARCHIVE_NAME):\n",
        "        print(\"Downloading dataset from %s (3 MB)\" % URL)\n",
        "        opener = urlopen(URL)\n",
        "        with open(ARCHIVE_NAME, 'wb') as archive:\n",
        "            archive.write(opener.read())\n",
        "\n",
        "    print(\"Decompressing %s\" % ARCHIVE_NAME)\n",
        "    with closing(tarfile.open(ARCHIVE_NAME, \"r:gz\")) as archive:\n",
        "        archive.extractall(path='.')\n",
        "    os.remove(ARCHIVE_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
        "    # block to be able to use a multi-core grid search that also works under\n",
        "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
        "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
        "    # that is used when n_jobs != 1 in GridSearchCV\n",
        "\n",
        "    # the training data folder must be passed as first argument\n",
        "\n",
        "    path = os.path.abspath(\"txt_sentoken\")\n",
        "    dataset = load_files(path, shuffle=False)\n",
        "    print(\"n_samples: %d\" % len(dataset.data))\n",
        "\n",
        "    # split the dataset in training and test set:\n",
        "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
        "\n",
        "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
        "    # that are too rare or too frequent\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
        "        ('clf', LinearSVC(C=1000)),\n",
        "    ])\n",
        "\n",
        "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
        "    # more useful.\n",
        "    # Fit the pipeline on the training set using grid search for the parameters\n",
        "    parameters = {\n",
        "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "    }\n",
        "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
        "    grid_search.fit(docs_train, y_train)\n",
        "\n",
        "    # TASK: print the mean and std for each candidate along with the parameter\n",
        "    # settings for all the candidates explored by grid search.\n",
        "    n_candidates = len(grid_search.cv_results_['params'])\n",
        "    for i in range(n_candidates):\n",
        "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
        "                 % (grid_search.cv_results_['params'][i],\n",
        "                    grid_search.cv_results_['mean_test_score'][i],\n",
        "                    grid_search.cv_results_['std_test_score'][i]))\n",
        "\n",
        "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
        "    # named y_predicted\n",
        "    y_predicted = grid_search.predict(docs_test)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(metrics.classification_report(y_test, y_predicted,\n",
        "                                        target_names=dataset.target_names))\n",
        "\n",
        "    # Print and plot the confusion matrix\n",
        "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
        "    print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdMxuPsGZBJo",
        "outputId": "9e4d1741-d6cc-4de4-c90d-3fb37b81defe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_samples: 2000\n",
            "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.01\n",
            "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.85; std - 0.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.89      0.87      0.88       251\n",
            "         pos       0.87      0.90      0.88       249\n",
            "\n",
            "    accuracy                           0.88       500\n",
            "   macro avg       0.88      0.88      0.88       500\n",
            "weighted avg       0.88      0.88      0.88       500\n",
            "\n",
            "[[219  32]\n",
            " [ 26 223]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.matshow(cm)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "oNb25jrGZDva",
        "outputId": "00b50448-ed4a-44d9-bb34-c44863199e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1cVyJKHQtfV"
      },
      "source": [
        "## Problem 2 (10 points): Explore the scikit-learn TfidVectorizer class\n",
        "\n",
        "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n",
        "* Define the term frequency–inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
        "* Run the TfidVectorizer class on the training data above (docs_train).\n",
        "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
        "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGCOW1yjQtfW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "#TF-IDF is a statistic that reflects how important a word is to a group of documents\n",
        "\n",
        "#play around with parameter of TfidfVectorizer (i.e. max_df = 2, min_df = 0.5, ngram_range = (1,2))\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True) \n",
        "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the first vector out (for the first document) \n",
        "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
        "# place tf-idf values in a pandas data frame \n",
        "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
        "df.sort_values(by=['tfidf'], ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "fOJnzTEOrD7d",
        "outputId": "bcdaec2c-24b3-4c37-9922-a435ecb204f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-85c69e04-a665-43b8-8187-9df16293b855\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.357976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bats</th>\n",
              "      <td>0.339542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bat</th>\n",
              "      <td>0.295216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.209848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>town</th>\n",
              "      <td>0.179439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>finchers</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fincher</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>finch</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>financing</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zzzzzzz</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35248 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85c69e04-a665-43b8-8187-9df16293b855')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85c69e04-a665-43b8-8187-9df16293b855 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85c69e04-a665-43b8-8187-9df16293b855');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              tfidf\n",
              "the        0.357976\n",
              "bats       0.339542\n",
              "bat        0.295216\n",
              "to         0.209848\n",
              "town       0.179439\n",
              "...             ...\n",
              "finchers   0.000000\n",
              "fincher    0.000000\n",
              "finch      0.000000\n",
              "financing  0.000000\n",
              "zzzzzzz    0.000000\n",
              "\n",
              "[35248 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWQAJmsQtfW"
      },
      "source": [
        "## Problem 3 (15 points): Machine learning algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNt6Ue6-QtfW"
      },
      "source": [
        "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
        "    * \"fit\" your TfidfVectorizer using docs_train\n",
        "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
        "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
        "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
        "* Examine two classifiers provided by scikit-learn \n",
        "    * LinearSVC\n",
        "    * KNeighborsClassifier\n",
        "    * Why do you think it might be working better?\n",
        "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
        "    * Can you conjecture on why the classifier made a mistake for this prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m945iZIxQtfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d2b12b-bb55-4309-96a6-35cbc8785c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xtrain Shape\n",
            "(1500, 325236)\n",
            "Xtest Shape\n",
            "(500, 325236)\n",
            "Y test Prediction 1 - LinearSVC\n",
            "[[163  88]\n",
            " [ 55 194]]\n",
            "Y test Prediction 2 - KNeighbors classifier\n",
            "[[ 86 165]\n",
            " [ 31 218]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True, max_df = 1, ngram_range = (1,2)) \n",
        "\n",
        "vectorizer_fit = vectorizer.fit(docs_train)\n",
        "\n",
        "Xtrain = vectorizer_fit.transform(docs_train)\n",
        "print(\"Xtrain Shape\")\n",
        "print(Xtrain.shape)\n",
        "\n",
        "Xtest = vectorizer_fit.transform(docs_test)\n",
        "print(\"Xtest Shape\")\n",
        "print(Xtest.shape)\n",
        "\n",
        "\n",
        "classifier_one = LinearSVC()\n",
        "classifier_one.fit(Xtrain, y_train)\n",
        "\n",
        "\n",
        "classifier_two = KNeighborsClassifier(n_neighbors=50, weights=\"distance\")\n",
        "classifier_two.fit(Xtrain, y_train)\n",
        "\n",
        "\n",
        "prediction_one = classifier_one.predict(Xtest)\n",
        "prediction_two = classifier_two.predict(Xtest)\n",
        "\n",
        "print(\"Y test Prediction 1 - LinearSVC\")\n",
        "print(metrics.confusion_matrix(y_test, prediction_one))\n",
        "print(\"Y test Prediction 2 - KNeighbors classifier\")\n",
        "print(metrics.confusion_matrix(y_test, prediction_two))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LinearSVC seems to have a better performance than KNeighbors because it had lower numbers of false positives and false negatives combined and a more even percentage of corrected positives and negatives. KNeighbors is heavily skewed towards predicting negatives. \n",
        "\n"
      ],
      "metadata": {
        "id": "h0-mubgpm2LH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoicR0YeQx5x"
      },
      "source": [
        "## Problem 4 (15 points): Use a Multi-Layer Perceptron (MLP) for classifying the reviews.  Explore the parameters for the MLP and compare the accuracies against your baseline algorithms in Problem 1.\n",
        "\n",
        "**Read the documentation for the MLPClassifier class at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.** \n",
        "* Note: This is *very similar* to using the LinearSVC and KNeighborsClassifier classes above!\n",
        "* Try different values for \"hidden_layer_sizes\".  What do you observe in terms of accuracy?\n",
        "* Try different values for \"activation\". What do you observe in terms of accuracy?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaDYg8XNQx5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f307a2-59ed-4296-b9ee-64bb8e3c0f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer size: 50, Activation String: identity\n",
            "[[215  40]\n",
            " [ 42 203]]\n",
            "Layer size: 50, Activation String: logistic\n",
            "[[213  42]\n",
            " [ 41 204]]\n",
            "Layer size: 50, Activation String: tanh\n",
            "[[215  40]\n",
            " [ 43 202]]\n",
            "Layer size: 50, Activation String: relu\n",
            "[[216  39]\n",
            " [ 41 204]]\n",
            "Layer size: 100, Activation String: identity\n",
            "[[215  40]\n",
            " [ 43 202]]\n",
            "Layer size: 100, Activation String: logistic\n",
            "[[214  41]\n",
            " [ 41 204]]\n",
            "Layer size: 100, Activation String: tanh\n",
            "[[215  40]\n",
            " [ 43 202]]\n",
            "Layer size: 100, Activation String: relu\n",
            "[[213  42]\n",
            " [ 43 202]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 0.0025)\n",
        "vectorizer_fit = vectorizer.fit(docs_train)\n",
        "Xtrain = vectorizer_fit.transform(docs_train)\n",
        "Xtest = vectorizer_fit.transform(docs_test)\n",
        "\n",
        "layer_sizes = [50, 100]\n",
        "activation_strings = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
        "\n",
        "for layer in layer_sizes:\n",
        "    for activation_string in activation_strings:\n",
        "      classifier = MLPClassifier(hidden_layer_sizes=(layer,), activation = activation_string)\n",
        "      classifier.fit(Xtrain, y_train)\n",
        "      y_pred = classifier.predict(Xtest)\n",
        "      print(\"Layer size: \" + str(layer) + \", Activation String: \" + activation_string)\n",
        "      print(metrics.confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G65MbRfQx5y"
      },
      "source": [
        "## Problem 5 (10 points): Accuracy is not everything!  How fast are the algorithms versus their accuracy?\n",
        "**Compare the runtime of your  baseline algorithms to the runtime of the MLPClassifier** \n",
        "\n",
        "**The jupyter command %timeit can be used to measure how long a calculation takes https://ipython.readthedocs.io/en/stable/interactive/magics.html.**\n",
        "* Try different values for \"hidden_layer_sizes\".  What do you observe in terms of runtime?\n",
        "* Try different values for \"activation\". What do you observe in terms of runtime?\n",
        "* How long does the \"fit\" function take as opposed to the \"predict\" function?  Can you explain why?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in layer_sizes:\n",
        "    for activation_string in activation_strings:\n",
        "      classifier = MLPClassifier(hidden_layer_sizes=(layer,), activation = activation_string)\n",
        "      print(\"Layer size: \" + str(layer) + \", Activation String: \" + activation_string )\n",
        "      %timeit classifier.fit(Xtrain, y_train)\n",
        "      %timeit classifier.predict(Xtest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-dGCFIgMze8",
        "outputId": "d75fe417-a097-4152-c322-a7d1b08b8e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer size: 50, Activation String: identity\n",
            "1 loop, best of 5: 17.1 s per loop\n",
            "100 loops, best of 5: 10.7 ms per loop\n",
            "Layer size: 50, Activation String: logistic\n",
            "1 loop, best of 5: 32.4 s per loop\n",
            "100 loops, best of 5: 11.3 ms per loop\n",
            "Layer size: 50, Activation String: tanh\n",
            "1 loop, best of 5: 18.3 s per loop\n",
            "100 loops, best of 5: 11.9 ms per loop\n",
            "Layer size: 50, Activation String: relu\n",
            "1 loop, best of 5: 18.3 s per loop\n",
            "100 loops, best of 5: 11 ms per loop\n",
            "Layer size: 100, Activation String: identity\n",
            "1 loop, best of 5: 29.8 s per loop\n",
            "10 loops, best of 5: 20.1 ms per loop\n",
            "Layer size: 100, Activation String: logistic\n",
            "1 loop, best of 5: 53.8 s per loop\n",
            "10 loops, best of 5: 22.3 ms per loop\n",
            "Layer size: 100, Activation String: tanh\n",
            "1 loop, best of 5: 29 s per loop\n",
            "10 loops, best of 5: 23.7 ms per loop\n",
            "Layer size: 100, Activation String: relu\n",
            "1 loop, best of 5: 28.5 s per loop\n",
            "10 loops, best of 5: 20.1 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "o0bcrrUzQtfX"
      },
      "source": [
        "\n",
        "## Problem 6 (20 points): Business question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8VZdOxfQtfX"
      },
      "source": [
        "* Suppose you had a machine learning algorithm that could detect the sentiment of tweets that was highly accurate.  What kind of business could you build around that?\n",
        "* Who would be your competitors, and what are their sizes?\n",
        "* What would be the size of the market for your product?\n",
        "* In addition, assume that your machine learning was slow to train, but fast in making predictions on new data.  How would that affect your business plan?\n",
        "* How could you use the cloud to support your product?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-g3psMyQtfX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "623bfcfa-b647-4054-c020-6a22737ce0dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nSuppose you had a machine learning algorithm that could detect the sentiment of tweets that was highly accurate. \\nWhat kind of business could you build around that?\\nWe could build a business where our product monitors feelings on social media for companies to gage customer opinions/reviews. \\nGaging customer opinions is important for most businesses to monitor their performance, especially amongst other rival companies, \\nincrease customer satisfaction, maintain a good public image and optimize marketing. \\n\\nWho would be your competitors, and what are their sizes?\\nOur competitors would be companies with SaaS tools like MonkeyLearn (28 employees, $2.88 M in funding, $5+M in revenue per annum), \\nIBM Watson (5290+ companies using, 7000+ employees, $1B+ valuation), Lexalytics (25-100 employees, 350K+funding), \\nAmazon Comprehend($1B+ in revenue, 10000+ employees) among others. Our competitors are well-established companies. \\n\\nWhat would be the size of the market for your product?\\nThe market for our product would be any company/organization/individual with a decently sized twitter following.\\n\\nIn addition, assume that your machine learning was slow to train, but fast in making predictions on new data. How would that affect your business plan?\\nIt would negatively affect our business plan, since twitter is fast-changing platform\\nand it's always important for businesses to keep up with social media and current views and trends. \\nTo counter this issue, a few factors to consider that can make the training time run faster are  changing the optimization algorithm and \\ntesting data structures. We can also take advantage of the fact that it is fast in making predictions on new data. \\n\\n\\nHow could you use the cloud to support your product?\\nThe cloud helps make machine learning more affordable. They provide cheap data storage as well as computational power. \\nSince we will be working with real-time data and the need for computational power will vary depending on the traffic on the internet, \\nthe time of the day etc, using the cloud will be more beneficial and efficient than trying to set up our own servers. \\n\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "\"\"\"\n",
        "Suppose you had a machine learning algorithm that could detect the sentiment of tweets that was highly accurate. \n",
        "What kind of business could you build around that?\n",
        "We could build a business where our product monitors feelings on social media for companies to gage customer opinions/reviews. \n",
        "Gaging customer opinions is important for most businesses to monitor their performance, especially amongst other rival companies, \n",
        "increase customer satisfaction, maintain a good public image and optimize marketing. \n",
        "\n",
        "Who would be your competitors, and what are their sizes?\n",
        "Our competitors would be companies with SaaS tools like MonkeyLearn (28 employees, $2.88 M in funding, $5+M in revenue per annum), \n",
        "IBM Watson (5290+ companies using, 7000+ employees, $1B+ valuation), Lexalytics (25-100 employees, 350K+funding), \n",
        "Amazon Comprehend($1B+ in revenue, 10000+ employees) among others. Our competitors are well-established companies. \n",
        "\n",
        "What would be the size of the market for your product?\n",
        "The market for our product would be any company/organization/individual with a decently sized twitter following.\n",
        "\n",
        "In addition, assume that your machine learning was slow to train, but fast in making predictions on new data. How would that affect your business plan?\n",
        "It would negatively affect our business plan, since twitter is fast-changing platform\n",
        "and it's always important for businesses to keep up with social media and current views and trends. \n",
        "To counter this issue, a few factors to consider that can make the training time run faster are  changing the optimization algorithm and \n",
        "testing data structures. We can also take advantage of the fact that it is fast in making predictions on new data. \n",
        "\n",
        "\n",
        "How could you use the cloud to support your product?\n",
        "The cloud helps make machine learning more affordable. They provide cheap data storage as well as computational power. \n",
        "Since we will be working with real-time data and the need for computational power will vary depending on the traffic on the internet, \n",
        "the time of the day etc, using the cloud will be more beneficial and efficient than trying to set up our own servers. \n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhHQoH_KQtfX"
      },
      "source": [
        "# Slides (for a 5-8 minute presentation) (20 points)\n",
        "\n",
        "\n",
        "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
        "\n",
        "2. (10 points) Communicating Results (figure/table)\n",
        "\n",
        "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl5sy8EUQtfY"
      },
      "source": [
        "\n",
        "# Done\n",
        "\n",
        "All set! \n",
        "\n",
        "** What do you need to submit?**\n",
        "\n",
        "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
        "\n",
        "\n",
        "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
        "\n",
        "* **Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
        "    * What data you collected? \n",
        "    * Why this topic is interesting or important to you? (Motivations)\n",
        "    * How did you analyse the data?\n",
        "    * What did you find in the data? \n",
        " \n",
        "     (please include figures or tables in the report, but no source code)\n",
        "\n",
        "\n",
        "*Please compress all the files into a single zipped file.*\n",
        "\n",
        "\n",
        "** How to submit: **\n",
        "\n",
        "        Please submit through canvas.wpi.edu\n",
        "\n",
        "### DS3010 Case Study 3 Team 9\n",
        "\n",
        "        \n",
        "** Note: Each team just needs to submits one submission **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HylAvN1gQtfY"
      },
      "source": [
        "# Grading Criteria:\n",
        "\n",
        "**Total Points: 100**\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Notebook results:**\n",
        "    Points: 80\n",
        "\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 1:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "    \n",
        "    -----------------------------------\n",
        "    Question 2:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "        \n",
        "    -----------------------------------\n",
        "    Question 3:\n",
        "    Points: 15 \n",
        "    -----------------------------------\n",
        "  \n",
        "    -----------------------------------\n",
        "    Question 4:  \n",
        "    Points: 15\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 5:  \n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 6:  \n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Slides (for a 5-8 minute presentation): Story-telling**\n",
        "    Points: 20\n",
        "\n",
        "\n",
        "1. Motivation about the data collection, why the topic is interesting to you.\n",
        "    Points: 5 \n",
        "\n",
        "2. Communicating Results (figure/table)\n",
        "    Points: 10 \n",
        "\n",
        "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
        "    Points: 5 \n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.2.0"
    },
    "colab": {
      "name": "CaseStudy3Group9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}